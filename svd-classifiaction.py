# -*- coding: utf-8 -*-
"""SVD_CLASS

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11G7VKN9LqLJI8ypL-MgJMmbrO__43zdd
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset

from tqdm import tqdm
from collections import defaultdict
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Check if CUDA is available
use_cuda = torch.cuda.is_available()

# If CUDA is available, use it; otherwise, use CPU
device = torch.device("cuda" if use_cuda else "cpu")

# Download NLTK stopwords if not already downloaded
import nltk
nltk.download('stopwords')
nltk.download('punkt')

# Define LSTM model for classification
class LSTMClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super(LSTMClassifier, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])  # Take the last time-step's output
        return out

print("Model defined")

# Load pre-trained word embeddings and word-to-index mapping
checkpoint = torch.load("svd-word-vectors.pt", map_location=torch.device('cpu'))
word_embeddings_tensor = checkpoint['word_embeddings']
word_to_index = checkpoint['word_to_index']

# Move word embeddings tensor to the device
word_embeddings_tensor = word_embeddings_tensor.to(device)

# Define a function to convert text to indices using word-to-index mapping
def text_to_indices(text):
    return [word_to_index[word] if word in word_to_index else 0 for word in text.split()]

# Load News Classification Dataset without preprocessing
train_data = pd.read_csv("/kaggle/working/preprocessed_train.csv")
test_data = pd.read_csv("test.csv")

print("Data loaded")
test_data['Description'] = test_data['Description'].str.lower()
# Convert text descriptions to sequences of indices
train_data['Description'] = train_data['Description'].apply(text_to_indices)
test_data['Description'] = test_data['Description'].apply(text_to_indices)

# Define custom dataset
class NewsDataset(Dataset):
    def __init__(self, descriptions, labels):
        self.descriptions = descriptions
        self.labels = labels

    def __len__(self):
        return len(self.descriptions)

    def __getitem__(self, idx):
        description = self.descriptions[idx]
        padded_description = self.pad_sequence(description)
        return torch.tensor(padded_description), torch.tensor(self.labels[idx]-1)

    def pad_sequence(self, sequence, max_length=100):
        if len(sequence) >= max_length:
            return sequence[:max_length]
        else:
            padding = [0] * (max_length - len(sequence))
            return sequence + padding


# Create DataLoader for training and validation sets
batch_size = 32
train_dataset = NewsDataset(train_data['Description'].tolist(), train_data['Class Index'].tolist())
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_dataset = NewsDataset(test_data['Description'].tolist(), test_data['Class Index'].tolist())
test_loader = DataLoader(test_dataset, batch_size=batch_size)

print("Data loaders created")

# Initialize LSTM classifier
input_size = word_embeddings_tensor.shape[1]
hidden_size = 128
# Determine the number of unique classes in the training data
num_classes = len(torch.unique(torch.tensor(train_data['Class Index'].tolist())))
print(num_classes)

# Initialize LSTM classifier with adjusted output size
output_size = num_classes
lstm_classifier = LSTMClassifier(input_size, hidden_size, output_size).to(device)

print(output_size)

print("LSTM classifier initialized")

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(lstm_classifier.parameters(), lr=0.001)

# Training the LSTM model
num_epochs = 10
for epoch in range(num_epochs):
    lstm_classifier.train()
    running_loss = 0.0
    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):
        inputs = word_embeddings_tensor[inputs.to(device)].float()  # Convert to float32
        inputs = inputs.to(device)
        labels = labels.to(device)
        optimizer.zero_grad()
        outputs = lstm_classifier(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * inputs.size(0)
    epoch_loss = running_loss / len(train_dataset)
    print(f'Training Loss: {epoch_loss:.4f}')

# Evaluation on test set
lstm_classifier.eval()
correct = 0
total = 0
with torch.no_grad():
    for inputs, labels in test_loader:
        inputs = word_embeddings_tensor[inputs.to(device)].float()  # Convert to float32
        inputs = inputs.to(device)
        labels = labels.to(device)
        outputs = lstm_classifier(inputs)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = correct / total
print(f'Test Accuracy: {accuracy:.4f}')

# Save the trained model
torch.save(lstm_classifier.state_dict(), "svd-classification-model.pt")
print("Model saved successfully.")

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix

# Function to compute performance metrics
def compute_metrics(model, dataloader):
    model.eval()
    y_true = []
    y_pred = []
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = word_embeddings_tensor[inputs.to(device)].float()  # Convert to float32
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())

    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average='weighted')
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    cm = confusion_matrix(y_true, y_pred)

    return acc, f1, precision, recall, cm

# Compute metrics for SVD model
train_acc_svd, train_f1_svd, train_precision_svd, train_recall_svd, train_cm_svd = compute_metrics(lstm_classifier, train_loader)
test_acc_svd, test_f1_svd, test_precision_svd, test_recall_svd, test_cm_svd = compute_metrics(lstm_classifier, test_loader)


print("SVD Model Performance Metrics:")
print("Train Accuracy:", train_acc_svd)
print("Train F1 Score:", train_f1_svd)
print("Train Precision:", train_precision_svd)
print("Train Recall:", train_recall_svd)
print("Train Confusion Matrix:")
print(train_cm_svd)

print("\nTest Accuracy:", test_acc_svd)
print("Test F1 Score:", test_f1_svd)
print("Test Precision:", test_precision_svd)
print("Test Recall:", test_recall_svd)
print("Test Confusion Matrix:")
print(test_cm_svd)

import matplotlib.pyplot as plt
import seaborn as sns

# Function to plot confusion matrix
def plot_confusion_matrix(cm, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
    plt.title(title)
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.show()

# Plot confusion matrix for SVD model
plot_confusion_matrix(train_cm_svd, "SVD Train Confusion Matrix")
plot_confusion_matrix(test_cm_svd, "SVD Test Confusion Matrix")

