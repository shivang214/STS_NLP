# -*- coding: utf-8 -*-
"""SKIPGRAM_SIRF

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XcLeLvo_cGQPs_8NS1Li4ybRXNLbmLae
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import Counter
from torch.utils.data import Dataset, DataLoader
import nltk
from nltk.corpus import stopwords

# Download NLTK stopwords if not already downloaded
nltk.download('stopwords')

# Assuming train.csv has a column named 'Description', change the path accordingly
train_data = pd.read_csv('/kaggle/working/preprocessed_train.csv')
corpus = train_data['Description'].tolist()

print("Loaded")

corpus_subset = corpus[:20000]

print("Preprocessed")

class Word2VecDataset(Dataset):
    def __init__(self, corpus, window_size=5, negative_samples=2):
        self.corpus = corpus
        self.window_size = window_size
        self.negative_samples = negative_samples
        self.build_vocabulary()
        self.build_dataset()

        print("Initialized")

    def build_vocabulary(self):
        word_counts = Counter()
        for sentence in self.corpus:
            word_counts.update(sentence.split())
        self.vocab = sorted(word_counts, key=word_counts.get, reverse=True)
        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}
        self.idx2word = {idx: word for word, idx in self.word2idx.items()}
        self.vocab_size = len(self.vocab)
        print(self.vocab_size)

    def build_dataset(self):
        print("Inside build")
        self.training_data = []
        for sentence in self.corpus:
            words = sentence.split()
            for i, target_word in enumerate(words):
                target_index = self.word2idx[target_word]
                context_indices = [self.word2idx.get(words[j], 0) for j in range(max(0, i - self.window_size), min(len(words), i + self.window_size + 1))]
                context_indices.remove(target_index)
                self.training_data.extend([(target_index, context_index, 1) for context_index in context_indices])
                # Negative sampling
                for _ in range(self.negative_samples):
                    negative_index = np.random.randint(0, self.vocab_size)
                    while negative_index == target_index:
                        negative_index = np.random.randint(0, self.vocab_size)
                    self.training_data.append((target_index, negative_index, 0))

    def __len__(self):
        return len(self.training_data)

    def __getitem__(self, idx):
        target_word, context_word, label = self.training_data[idx]
        return target_word, context_word, label

class SkipGramNS(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(SkipGramNS, self).__init__()
        self.target_embedding = nn.Embedding(vocab_size, embedding_dim)
        self.context_embedding = nn.Embedding(vocab_size, embedding_dim)
        print("Skip-gram initialized")

    def forward(self, target_word, context_word):
        target_embed = self.target_embedding(target_word)
        context_embed = self.context_embedding(context_word)
        scores = torch.sum(target_embed * context_embed, dim=1)
        return scores

# Hyperparameters
window_size = 2
negative_samples = 2
embedding_dim = 100
learning_rate = 0.01
batch_size = 64
epochs = 10

# Creating dataset and dataloader
dataset = Word2VecDataset(corpus_subset, window_size, negative_samples)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

print("Data loaded")

# Model, loss function, and optimizer
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = SkipGramNS(dataset.vocab_size, embedding_dim).to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(epochs):
    total_loss = 0
    for target_word, context_word, label in dataloader:
        target_word, context_word, label = target_word.to(device), context_word.to(device), label.to(device)
        optimizer.zero_grad()
        scores = model(target_word, context_word)
        loss = criterion(scores, label.float())
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader)}")

# Get word vectors
word_vectors = model.target_embedding.weight.detach().cpu()
word_to_index_mapping = dataset.word2idx

# Save word vectors along with word-to-index mapping
torch.save({'word_vectors': word_vectors, 'word_to_index_mapping': word_to_index_mapping}, 'skip-gram-word-vectors.pt')