# -*- coding: utf-8 -*-
"""SVD_SIRF

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lmt81hHpvF8xLmvu4AtADTaIkrNF53mr
"""

import numpy as np
import pandas as pd
from collections import defaultdict
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import torch
from scipy.sparse import lil_matrix
from scipy.sparse.linalg import svds

# Check if CUDA is available
use_cuda = torch.cuda.is_available()

# If CUDA is available, use it; otherwise, use CPU
device = torch.device("cuda" if use_cuda else "cpu")

# Download NLTK stopwords if not already downloaded
import nltk
nltk.download('stopwords')
nltk.download('punkt')

# Read train.csv without preprocessing
data = pd.read_csv("/kaggle/working/preprocessed_train.csv")

# Initialize word_to_index dictionary
word_to_index = defaultdict(lambda: len(word_to_index))

# Preprocess all descriptions
preprocessed_descriptions = data['Description']

# Step 2: Build co-occurrence matrix
co_occurrence_matrix = defaultdict(lambda: defaultdict(int))

window_size = 5  # Reduce window size for memory optimization
vocab_count = defaultdict(int)  # Track word frequencies

for sentence in preprocessed_descriptions:
    words = sentence.split()
    for i, word in enumerate(words):
        word_index = word_to_index[word]
        vocab_count[word_index] += 1  # Increment word frequency
        start = max(0, i - window_size)
        end = min(len(words), i + window_size + 1)
        for j in range(start, end):
            if j != i:
                context_word = words[j]
                context_index = word_to_index[context_word]
                co_occurrence_matrix[word_index][context_index] += 1

# Convert the co-occurrence matrix to a sparse matrix
num_words = len(word_to_index)
co_occurrence_sparse = lil_matrix((num_words, num_words))
for word_index in co_occurrence_matrix:
    for context_index, count in co_occurrence_matrix[word_index].items():
        co_occurrence_sparse[word_index, context_index] = count

# Step 3: Perform Truncated SVD (TSVD) on the sparse matrix
num_dimensions = 100
U, S, Vt = svds(co_occurrence_sparse, k=num_dimensions)

# Construct word embeddings
word_embeddings = U @ np.diag(np.sqrt(S))

# Create word-to-index mapping for SVD embeddings
svd_word_to_index = {word: idx for idx, word in enumerate(word_to_index.keys())}

# Convert word embeddings to PyTorch tensor
word_embeddings_tensor = torch.tensor(word_embeddings, device=device)

# Save word embeddings and word-to-index mapping to a PyTorch file
torch.save({
    'word_embeddings': word_embeddings_tensor,
    'word_to_index': svd_word_to_index
}, "svd-word-vectors.pt")

print("Complete")