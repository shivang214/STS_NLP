# -*- coding: utf-8 -*-
"""SKIP_CLASS

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r3PMK8r9VYk_RQQOKMnQI0Xw6Ef-7iKU
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import pandas as pd
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from nltk.tokenize import word_tokenize

# Check if CUDA is available
use_cuda = torch.cuda.is_available()

# If CUDA is available, use it; otherwise, use CPU
device = torch.device("cuda" if use_cuda else "cpu")

# Define LSTM model for classification
class LSTMClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super(LSTMClassifier, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])  # Take the last time-step's output
        return out

# Load pre-trained word embeddings and word-to-index mapping
checkpoint = torch.load("skip-gram-word-vectors.pt", map_location=device)
word_embeddings_tensor = checkpoint['word_vectors']
word_to_index = checkpoint['word_to_index_mapping']

# Load train and test datasets
train_data = pd.read_csv("/kaggle/working/preprocessed_train.csv")
test_data = pd.read_csv("test.csv")

# Tokenize and convert text to indices
def text_to_indices(text):
    tokens = word_tokenize(text)
    indices = [word_to_index[word] if word in word_to_index else 0 for word in tokens]
    return indices


test_data['Description'] = test_data['Description'].str.lower()
train_data['Description'] = train_data['Description'].apply(text_to_indices)
test_data['Description'] = test_data['Description'].apply(text_to_indices)

# Define custom dataset
class NewsDataset(Dataset):
    def __init__(self, descriptions, labels):
        self.descriptions = descriptions
        self.labels = labels

    def __len__(self):
        return len(self.descriptions)

    def __getitem__(self, idx):
        description = self.descriptions[idx]
        padded_description = self.pad_sequence(description)
        return torch.tensor(padded_description), torch.tensor(self.labels[idx]-1)

    def pad_sequence(self, sequence, max_length=100):
        if len(sequence) >= max_length:
            return sequence[:max_length]
        else:
            padding = [0] * (max_length - len(sequence))
            return sequence + padding

# Create DataLoader for training set
batch_size = 32
train_dataset = NewsDataset(train_data['Description'].tolist(), train_data['Class Index'].tolist())
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Initialize LSTM classifier
input_size = word_embeddings_tensor.shape[1]
hidden_size = 128
num_classes = len(torch.unique(torch.tensor(train_data['Class Index'].tolist())))
output_size = num_classes
lstm_classifier = LSTMClassifier(input_size, hidden_size, output_size).to(device)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(lstm_classifier.parameters(), lr=0.001)

# Training the LSTM model
num_epochs = 10
for epoch in range(num_epochs):
    lstm_classifier.train()
    running_loss = 0.0
    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):
        inputs = word_embeddings_tensor[inputs.to(device)].float()  # Convert to float32
        inputs = inputs.to(device)
        labels = labels.to(device)
        optimizer.zero_grad()
        outputs = lstm_classifier(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * inputs.size(0)
    epoch_loss = running_loss / len(train_dataset)
    print(f'Training Loss: {epoch_loss:.4f}')

# Evaluation on test set
test_dataset = NewsDataset(test_data['Description'].tolist(), test_data['Class Index'].tolist())
test_loader = DataLoader(test_dataset, batch_size=batch_size)
lstm_classifier.eval()
correct = 0
total = 0
with torch.no_grad():
    for inputs, labels in test_loader:
        inputs = word_embeddings_tensor[inputs.to(device)].float()  # Convert to float32
        inputs = inputs.to(device)
        labels = labels.to(device)
        outputs = lstm_classifier(inputs)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = correct / total
print(f'Test Accuracy: {accuracy:.4f}')


torch.save(lstm_classifier.state_dict(), 'skip-gram-classification-model.pt')
print("Model saved as skip-gram-classification-model.pt")

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix

# Function to compute performance metrics
def compute_metrics(model, dataloader):
    model.eval()
    y_true = []
    y_pred = []
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = word_embeddings_tensor[inputs.to(device)].float()  # Convert to float32
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())

    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average='weighted')
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    cm = confusion_matrix(y_true, y_pred)

    return acc, f1, precision, recall, cm

train_acc_skipgram, train_f1_skipgram, train_precision_skipgram, train_recall_skipgram, train_cm_skipgram = compute_metrics(lstm_classifier, train_loader)
test_acc_skipgram, test_f1_skipgram, test_precision_skipgram, test_recall_skipgram, test_cm_skipgram = compute_metrics(lstm_classifier, test_loader)



print("\nSkip-gram Model Performance Metrics:")
print("Train Accuracy:", train_acc_skipgram)
print("Train F1 Score:", train_f1_skipgram)
print("Train Precision:", train_precision_skipgram)
print("Train Recall:", train_recall_skipgram)
print("Train Confusion Matrix:")
print(train_cm_skipgram)

print("\nTest Accuracy:", test_acc_skipgram)
print("Test F1 Score:", test_f1_skipgram)
print("Test Precision:", test_precision_skipgram)
print("Test Recall:", test_recall_skipgram)
print("Test Confusion Matrix:")
print(test_cm_skipgram)

import matplotlib.pyplot as plt
import seaborn as sns

# Function to plot confusion matrix
def plot_confusion_matrix(cm, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
    plt.title(title)
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.show()

# Plot confusion matrix for Skip-gram model
plot_confusion_matrix(train_cm_skipgram, "Skip-gram Train Confusion Matrix")
plot_confusion_matrix(test_cm_skipgram, "Skip-gram Test Confusion Matrix")

import matplotlib.pyplot as plt

# Define lists of metrics
accuracy_skipgram = [test_acc_skipgram, train_acc_skipgram]
precision_skipgram = [test_precision_skipgram, train_precision_skipgram]
recall_skipgram = [test_recall_skipgram, train_recall_skipgram]
f1_skipgram = [test_f1_skipgram, train_f1_skipgram]

# Define metrics names
metrics_names = ['Test', 'Train']

# Plotting
plt.figure(figsize=(15, 10))

plt.subplot(2, 2, 1)
plt.plot(metrics_names, accuracy_skipgram, marker='o')
plt.title('Accuracy - Skip-gram')
plt.xlabel('Performance on Test and Train Sets')

plt.subplot(2, 2, 2)
plt.plot(metrics_names, precision_skipgram, marker='o')
plt.title('Precision - Skip-gram')
plt.xlabel('Performance on Test and Train Sets')

plt.subplot(2, 2, 3)
plt.plot(metrics_names, recall_skipgram, marker='o')
plt.title('Recall - Skip-gram')
plt.xlabel('Performance on Test and Train Sets')

plt.subplot(2, 2, 4)
plt.plot(metrics_names, f1_skipgram, marker='o')
plt.title('F1 Score - Skip-gram')
plt.xlabel('Performance on Test and Train Sets')

plt.tight_layout()
plt.show()