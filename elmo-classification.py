# -*- coding: utf-8 -*-
"""classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mVRSPdEH4Et6PStUMr_kEsX3dWH3ycLX
"""

!conda install -y gdown

#train
!gdown --id 15_8L8Cn7rB9LnaBkwB7CW4qNqDlQqrSk

#test
!gdown --id 1EbcAMsJKVBlQFeoDSUmSUY7dHxOf67Jo

#word_to_idx
!gdown --id 1qJUfNJanTKZtE-DbgJYDa1oCHy_CMEPK

#forw
!gdown --id 12G4fmBy_wQZTT09N8ta-4Yhwodf5In7i

#back
!gdown --id 1egmqg702GaoaJCIkz75UeaulHf2aCAwo

import pandas as pd


file_path_train = 'preprocessed_train.csv'
train_df = pd.read_csv(file_path_train)

file_path_test = 'test.csv'
test_df = pd.read_csv(file_path_test)

import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')

import gensim.downloader as api
import numpy as np

# word_vectors = api.load('glove-twitter-100')

import pickle

with open('glove-100.pkl', 'rb') as f:
    embeddings_glove = pickle.load(f)

with open('word_2_idx.pkl', 'rb') as f:
    word_to_idx = pickle.load(f)
    idx_to_word = {i: char for char, i in word_to_idx.items()}

import numpy as np
import torch


embedding_dim = 100
pretrained_embeddings = 1 * np.random.randn(len(word_to_idx), embedding_dim)

for word, index in word_to_idx.items():
    if word in embeddings_glove.key_to_index:
        pretrained_embeddings[index] = torch.tensor(embeddings_glove[word], dtype=torch.float32)

pretrained_embeddings = torch.tensor(pretrained_embeddings, dtype=torch.float32)

import torch
import torch.nn as nn

class ElmoF(nn.Module):
    def __init__(self, embedding_dim = 100, hidden_dim = 100):
        super(ElmoF, self).__init__()
        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)

        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, 1, batch_first=True)
        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, 1, batch_first=True)
        self.output_layer = nn.Linear(hidden_dim, len(pretrained_embeddings))

    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out1, _ = self.lstm1(embedded)
        lstm_out2, _ = self.lstm2(lstm_out1)

        embs = lstm_out2[:,-1,:]
        output = self.output_layer(embs)

        return output

import torch
import torch.nn as nn

class ElmoB(nn.Module):
    def __init__(self, embedding_dim = 100, hidden_dim = 100):
        super(ElmoB, self).__init__()
        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)

        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, 1, batch_first=True)
        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, 1, batch_first=True)
        self.output_layer = nn.Linear(hidden_dim, len(pretrained_embeddings))

    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out1, _ = self.lstm1(embedded)
        lstm_out2, _ = self.lstm2(lstm_out1)

        embs = lstm_out2[:,-1,:]
        output = self.output_layer(embs)

        return output

fwd = ElmoF(embedding_dim=100, hidden_dim=100)
fwd.load_state_dict(torch.load("forward.pt"))
fwd.eval()

bwd = ElmoB(embedding_dim=100, hidden_dim=100)
bwd.load_state_dict(torch.load("backward.pt"))
bwd.eval()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import pandas as pd
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from nltk.tokenize import word_tokenize

class NewsDataset(Dataset):
    def __init__(self, dataframe):
        self.data = dataframe
        self.word_to_idx = word_to_idx

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        description = self.data.iloc[idx]['Description']
        class_index = self.data.iloc[idx]['Class Index'] - 1

        # Tokenize and convert words to indices
        tokenized_description = [self.word_to_idx.get(word, self.word_to_idx['<UNK>']) for word in description.split()]

        return {
            'inputs': tokenized_description,
            'label': class_index
        }

def custom_collate_fn(batch):
    descriptions = [item['inputs'] for item in batch]
    class_indices = [item['label'] for item in batch]

    max_len = max(len(description) for description in descriptions)
    padded_descriptions = [description + [word_to_idx['<PAD>']] * (max_len - len(description)) for description in descriptions]

    return {
        'inputs': torch.tensor(padded_descriptions),
        'label': torch.tensor(class_indices)
    }

dataset = NewsDataset(train_df)
train_loader = DataLoader(dataset, batch_size=64, collate_fn=custom_collate_fn, shuffle=True)

for i in train_loader:
    print(i)
    break

import torch.nn.functional as F

class Elmo(nn.Module):
    def __init__(self, embedding_dim=100, hidden_dim=100, lambdas = torch.tensor([0.33, 0.33, 0.33]), train_lambda=True):
        super(Elmo, self).__init__()
        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)
        self.f = fwd
        self.b = bwd
        self.lambdas = nn.Parameter(lambdas)
        self.lambdas.requires_grad = train_lambda
        self.f.requires_grad = False
        self.b.requires_grad = False
        self.lstm = nn.LSTM(hidden_dim, hidden_dim, 1, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(2 * hidden_dim, 4)

    def forward(self, x):
        embedded = self.embedding(x)
        lstm_fw1, _ = self.f.lstm1(embedded)
        lstm_fw2, _ = self.f.lstm2(lstm_fw1)

        lstm_bw1, _ = self.b.lstm1(embedded)
        lstm_bw2, _ = self.b.lstm2(lstm_bw1)

        lm = F.softmax(self.lambdas, dim=0)

        layer_0 = lm[0] * embedded
        layer_1 = lm[1] * (lstm_fw1 + lstm_bw1)
        layer_2 = lm[2] * (lstm_fw2 + lstm_bw2)

        final_combined = layer_0 + layer_1 + layer_2

        lstm_out, _ = self.lstm(final_combined)

        embs = lstm_out[:,-1,:]
        output = self.fc(embs)

        return output

# Define training loop
from tqdm import tqdm

def train(model, train_loader, optimizer, criterion, num_epochs=10):
    model.train()
    for epoch in range(num_epochs):
        total_loss = 0
        correct_predictions = 0
        total_samples = 0

        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}", leave=False):
            inputs = batch['inputs'].to(device)
            labels = batch['label'].to(device)

            optimizer.zero_grad()

            outputs = model(inputs)
            loss = criterion(outputs, labels)

            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            correct_predictions += (predicted == labels).sum().item()
            total_samples += labels.size(0)

        epoch_loss = total_loss / len(train_loader)
        accuracy = correct_predictions / total_samples * 100

        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%")

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = Elmo().to(device)
criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx['<PAD>'])
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
train(model, train_loader, optimizer, criterion, num_epochs=10)

print(model.lambdas)

"""## Fixed Lambdas"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = Elmo(lambdas=torch.tensor([0.1, 0.3, 0.6]), train_lambda=False).to(device)
criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx['<PAD>'])
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
train(model, train_loader, optimizer, criterion, num_epochs=10)

"""# TEST"""

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
test_dataset = NewsDataset(test_df)
test_loader = DataLoader(test_dataset, batch_size=64, collate_fn=custom_collate_fn)

# Function to evaluate the model and compute performance metrics
def evaluate(model, test_loader):
    model.eval()
    y_true = []
    y_pred = []

    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Evaluating"):
            inputs = batch['inputs'].to(device)
            labels = batch['label'].to(device)

            outputs = model(inputs)

            _, predicted = torch.max(outputs, 1)
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())

    acc = accuracy_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred, average='weighted')
    precision = precision_score(y_true, y_pred, average='weighted')
    recall = recall_score(y_true, y_pred, average='weighted')
    cm = confusion_matrix(y_true, y_pred)

    print("Test Accuracy:", acc)
    print("Test F1 Score:", f1)
    print("Test Precision:", precision)
    print("Test Recall:", recall)
    print("Test Confusion Matrix:")
    print(cm)

    return acc, f1, precision, recall, cm

test_acc, test_f1, test_precision, test_recall, test_cm = evaluate(model, test_loader)

import matplotlib.pyplot as plt
import seaborn as sns

def plot_confusion_matrix(cm, title):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
    plt.title(title)
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.show()

plot_confusion_matrix(train_cm," ")