# -*- coding: utf-8 -*-
"""elmo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f72O5FDg9NlLDkebhwbeg8DRKCVlztZ7
"""

!conda install -y gdown

#train
!gdown --id 15_8L8Cn7rB9LnaBkwB7CW4qNqDlQqrSk

#test
!gdown --id 1EbcAMsJKVBlQFeoDSUmSUY7dHxOf67Jo

import pandas as pd


file_path_train = 'preprocessed_train.csv'
train_df = pd.read_csv(file_path_train)

file_path_test = 'test.csv'
test_df = pd.read_csv(file_path_test)

import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')

import gensim.downloader as api
import numpy as np

# word_vectors = api.load('glove-twitter-100')

import pickle
# with open('glove-100.pkl', 'wb') as f:
#     pickle.dump(word_vectors, f)

with open('glove-100.pkl', 'rb') as f:
    embeddings_glove = pickle.load(f)

sentences=train_df['Description'].tolist()
tokenized_sentences=[word_tokenize(sentence) for sentence in sentences]

from collections import defaultdict

# word_count = defaultdict(int)

# for sentence in tokenized_sentences:
#     for word in sentence:
#         if word.lower() in embeddings_glove.key_to_index:
#             word_count[(word.lower())] += 1


# vocab = set(['<UNK>', '<PAD>'])

# for char, cnt in word_count.items():
#     vocab.add(char)

# print(f'Vocab Length: {len(vocab)}')

# word_to_idx = {char: i for i, char in enumerate(vocab)}
# idx_to_word = {i: char for char, i in word_to_idx.items()}

import pickle
# with open('word_2_idx.pkl', 'wb') as f:
#     pickle.dump(word_to_idx, f)

with open('word_2_idx.pkl', 'rb') as f:
    word_to_idx = pickle.load(f)
    idx_to_word = {i: char for char, i in word_to_idx.items()}

inpt = []
for sentence in train_df['Description']:
    tokens = [word_to_idx.get(word, word_to_idx['<UNK>']) for word in word_tokenize(sentence)]
    for i in range(1 , len(tokens)):
        n_grams = tokens[:i+1]
        inpt.append(n_grams)

print(len(inpt))

# Embeddings load from glove
# X, Y shape
# Elmo Forward, Backward Build.

import torch
from torch.nn.utils.rnn import pad_sequence

max_len = max(len(seq) for seq in inpt)

padded_sequences = [torch.tensor([word_to_idx['<PAD>']] * (max_len - len(seq)) + seq) for seq in inpt]

padded_sequences = pad_sequence(padded_sequences, batch_first=True, padding_value=word_to_idx['<PAD>'])

print(padded_sequences)

X, Y = padded_sequences[:,:-1], padded_sequences[:,-1]

X[0]

Y[0]

embedding_dim = 100
pretrained_embeddings = 1 * np.random.randn(len(word_to_idx), embedding_dim)

for word, index in word_to_idx.items():
    if word in embeddings_glove.key_to_index:
        pretrained_embeddings[index] = torch.tensor(embeddings_glove[word], dtype=torch.float32)

pretrained_embeddings = torch.tensor(pretrained_embeddings, dtype=torch.float32)

import torch
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split

X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.1, random_state=42)

train_dataset = TensorDataset(X_train, Y_train)
val_dataset = TensorDataset(X_val, Y_val)

batch_size = 256

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)

import torch
import torch.nn as nn

class ElmoF(nn.Module):
    def __init__(self, embedding_dim = 100, hidden_dim = 100):
        super(ElmoF, self).__init__()
        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)

        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, 1, batch_first=True)
        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, 1, batch_first=True)
        self.output_layer = nn.Linear(hidden_dim, len(pretrained_embeddings))

    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out1, _ = self.lstm1(embedded)
        lstm_out2, _ = self.lstm2(lstm_out1)

        embs = lstm_out2[:,-1,:]
        output = self.output_layer(embs)

        return output

import torch
import torch.nn as nn

class ElmoB(nn.Module):
    def __init__(self, embedding_dim = 100, hidden_dim = 100):
        super(ElmoB, self).__init__()
        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)

        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, 1, batch_first=True)
        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, 1, batch_first=True)
        self.output_layer = nn.Linear(hidden_dim, len(pretrained_embeddings))

    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out1, _ = self.lstm1(embedded)
        lstm_out2, _ = self.lstm2(lstm_out1)

        embs = lstm_out2[:,-1,:]
        output = self.output_layer(embs)

        return output

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = ElmoF().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

epochs = 5

# Training loop
for epoch in range(epochs):
    model.train()
    train_loss = 0.0

    # Iterate over the training data
    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}', leave=False):
        inputs, labels = inputs.to(device), labels.to(device)

        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(inputs)

        # Calculate loss
        loss = criterion(outputs, labels)

        # Backward pass
        loss.backward()

        # Update weights
        optimizer.step()

        # Update training loss
        train_loss += loss.item() * inputs.size(0)

    # Calculate average training loss
    train_loss = train_loss / len(train_loader.dataset)

    # Validation loop
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        # Iterate over the validation data
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            # Forward pass
            outputs = model(inputs)

            # Calculate loss
            loss = criterion(outputs, labels)

            # Update validation loss
            val_loss += loss.item() * inputs.size(0)

            # Compute accuracy
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    # Calculate average validation loss
    val_loss = val_loss / len(val_loader.dataset)

    # Calculate validation accuracy
    val_accuracy = correct / total

    # Print training and validation loss
    print(f'Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')

torch.save(model.state_dict(), 'forward.pt')

"""# **BACK**"""

inpt = []
for sentence in train_df['Description']:
    tokens = [word_to_idx.get(word, word_to_idx['<UNK>']) for word in word_tokenize(sentence)]
    for i in range(1 , len(tokens)):
        n_grams = tokens[-1 * (i+1) :]
        inpt.append(n_grams[::-1])

print(len(inpt))

import torch
from torch.nn.utils.rnn import pad_sequence

max_len = max(len(seq) for seq in inpt)

padded_sequences = [torch.tensor([word_to_idx['<PAD>']] * (max_len - len(seq)) + seq) for seq in inpt]

padded_sequences = pad_sequence(padded_sequences, batch_first=True, padding_value=word_to_idx['<PAD>'])

print(padded_sequences)

X, Y = padded_sequences[:,:-1], padded_sequences[:,-1]

import torch
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split

X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.1, random_state=42)

train_dataset = TensorDataset(X_train, Y_train)
val_dataset = TensorDataset(X_val, Y_val)

batch_size = 256

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = ElmoB().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

epochs = 5

# Training loop
for epoch in range(epochs):
    model.train()
    train_loss = 0.0

    # Iterate over the training data
    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}', leave=False):
        inputs, labels = inputs.to(device), labels.to(device)

        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(inputs)

        # Calculate loss
        loss = criterion(outputs, labels)

        # Backward pass
        loss.backward()

        # Update weights
        optimizer.step()

        # Update training loss
        train_loss += loss.item() * inputs.size(0)

    # Calculate average training loss
    train_loss = train_loss / len(train_loader.dataset)

    # Validation loop
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        # Iterate over the validation data
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            # Forward pass
            outputs = model(inputs)

            # Calculate loss
            loss = criterion(outputs, labels)

            # Update validation loss
            val_loss += loss.item() * inputs.size(0)

            # Compute accuracy
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    # Calculate average validation loss
    val_loss = val_loss / len(val_loader.dataset)

    # Calculate validation accuracy
    val_accuracy = correct / total

    # Print training and validation loss
    print(f'Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')

torch.save(model.state_dict(), 'backward.pt')

